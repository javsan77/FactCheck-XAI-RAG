{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZA7YwN6fg+jUqSvKOvbpr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["### 1. ConfiguraciÃ³n Inicial de Colab"],"metadata":{"id":"tzvTv3RlaX5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"E5UAFQr0RP4B"},"outputs":[],"source":["# 1. Instalar dependencias principales\n","!pip install transformers torch accelerate\n","!pip install langchain langchain-community\n","!pip install chromadb sentence-transformers\n","!pip install fastapi uvicorn pyngrok\n","!pip install gradio pandas numpy\n","!pip install shap lime-tabular\n","!pip install arxiv requests beautifulsoup4"]},{"cell_type":"code","source":["### 2. Importaciones y ConfiguraciÃ³n"],"metadata":{"id":"OnE7OAq3abMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from typing import List, Dict, Any, Optional\n","import json\n","import re\n","from datetime import datetime"],"metadata":{"id":"YFNDUXBe9A8l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LangChain y RAG\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.document_loaders import TextLoader\n","from langchain.schema import Document"],"metadata":{"id":"SmJo_3FE-iMh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So far so good. Main Dependencies were installed Ok."],"metadata":{"id":"EY0I2-cTTnTZ"}},{"cell_type":"code","source":["# Transformers para LLM\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","import torch"],"metadata":{"id":"_1LOITna-xys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explicabilidad\n","!pip install shap\n","!pip install lime\n","\n","import shap\n","from lime.lime_text import LimeTextExplainer"],"metadata":{"collapsed":true,"id":"nXvzd2NAYvI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# API y Frontend\n","import gradio as gr\n","from pyngrok import ngrok"],"metadata":{"id":"UMxlj8bSZ4f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Utilidades\n","import requests\n","from bs4 import BeautifulSoup\n","import arxiv\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"SeQD2iXfZ92m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 1: PreparaciÃ³n de Datos CientÃ­ficos"],"metadata":{"id":"BvwkClKhagix"}},{"cell_type":"markdown","source":["### 3. Descarga de Corpus CientÃ­fico"],"metadata":{"id":"CMDqAgrLafOV"}},{"cell_type":"code","source":["class ScientificDataCollector:\n","    def __init__(self):\n","        self.documents = []\n","        self.tortured_phrases = {\n","            \"bosom malignancy\": \"breast cancer\",\n","            \"corpus luteum\": \"corpus luteum\",  # correcto\n","            \"fake example\": \"real term\"\n","        }\n","\n","    def collect_arxiv_papers(self, query: str, max_results: int = 20):\n","        \"\"\"Descarga papers de ArXiv en biomedicina\"\"\"\n","        client = arxiv.Client()\n","        search = arxiv.Search(\n","            query=f\"{query} AND cat:q-bio*\",\n","            max_results=max_results,\n","            sort_by=arxiv.SortCriterion.Relevance\n","        )\n","\n","        papers = []\n","        for result in client.results(search):\n","            paper = {\n","                'title': result.title,\n","                'authors': [author.name for author in result.authors],\n","                'abstract': result.summary,\n","                'url': result.pdf_url,\n","                'published': result.published.strftime('%Y-%m-%d'),\n","                'categories': result.categories\n","            }\n","            papers.append(paper)\n","\n","        return papers\n","\n","    def create_synthetic_contaminated_data(self):\n","        \"\"\"Crea datos sintÃ©ticos con 'tortured phrases'\"\"\"\n","        contaminated_texts = [\n","            \"The study of bosom malignancy has shown significant progress in recent years. Traditional treatments for bosom malignancy include chemotherapy and radiation.\",\n","            \"Machine learning models can help detect fake patterns in medical imaging, improving diagnostic accuracy.\",\n","            \"Recent research in artificial intelligence applications for medical diagnosis shows promising results.\"\n","        ]\n","\n","        clean_texts = [\n","            \"Breast cancer research has advanced significantly with new immunotherapy approaches. Clinical trials show improved survival rates.\",\n","            \"Deep learning algorithms demonstrate high accuracy in medical image analysis for early disease detection.\",\n","            \"Natural language processing techniques are being applied to clinical notes for better patient care.\"\n","        ]\n","\n","        return contaminated_texts + clean_texts\n"],"metadata":{"id":"ufGJxPKIaRId"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Recolectar datos\n","collector = ScientificDataCollector()"],"metadata":{"id":"V-ZOcdbvafsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datos de ArXiv (biomedicina)\n","print(\"ğŸ”„ Descargando papers de ArXiv...\")\n","arxiv_papers = collector.collect_arxiv_papers(\"cancer detection machine learning\", 15)"],"metadata":{"id":"vfr4P_eodwUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datos sintÃ©ticos contaminados\n","synthetic_data = collector.create_synthetic_contaminated_data()"],"metadata":{"id":"fzb6eMa5egDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"âœ… Recolectados {len(arxiv_papers)} papers de ArXiv\")\n","print(f\"âœ… Creados {len(synthetic_data)} documentos sintÃ©ticos\")"],"metadata":{"id":"Z3JutZk9e1S2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 4. Procesamiento y Chunking\n","\n","class DocumentPreprocessor:\n","    def __init__(self):\n","        self.text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=500,\n","            chunk_overlap=50,\n","            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n","        )\n","\n","    def process_arxiv_papers(self, papers: List[Dict]) -> List[Document]:\n","        \"\"\"Convierte papers de ArXiv a documentos LangChain\"\"\"\n","        documents = []\n","\n","        for paper in papers:\n","            # Combinar tÃ­tulo y abstract\n","            content = f\"Title: {paper['title']}\\n\\nAbstract: {paper['abstract']}\"\n","\n","            # Crear documento con metadata\n","            doc = Document(\n","                page_content=content,\n","                metadata={\n","                    'source': 'arxiv',\n","                    'title': paper['title'],\n","                    'authors': ', '.join(paper['authors']),\n","                    'url': paper['url'],\n","                    'published': paper['published'],\n","                    'categories': ', '.join(paper['categories'])\n","                }\n","            )\n","            documents.append(doc)\n","\n","        return documents\n","\n","    def process_synthetic_data(self, texts: List[str]) -> List[Document]:\n","        \"\"\"Procesa datos sintÃ©ticos\"\"\"\n","        documents = []\n","\n","        for i, text in enumerate(texts):\n","            doc = Document(\n","                page_content=text,\n","                metadata={\n","                    'source': 'synthetic',\n","                    'doc_id': f'synthetic_{i}',\n","                    'contaminated': 'bosom malignancy' in text or 'fake' in text\n","                }\n","            )\n","            documents.append(doc)\n","\n","        return documents\n","\n","    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n","        \"\"\"Divide documentos en chunks\"\"\"\n","        return self.text_splitter.split_documents(documents)\n"],"metadata":{"id":"OCXcc_lSfAcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Procesar documentos\n","preprocessor = DocumentPreprocessor()"],"metadata":{"id":"bXzVFkuQgTMR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"ğŸ”„ Procesando documentos...\")\n","arxiv_docs = preprocessor.process_arxiv_papers(arxiv_papers)\n","synthetic_docs = preprocessor.process_synthetic_data(synthetic_data)"],"metadata":{"id":"ns77F75Ugcb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_documents = arxiv_docs + synthetic_docs\n","chunked_docs = preprocessor.chunk_documents(all_documents)\n","\n","print(f\"âœ… Total documentos: {len(all_documents)}\")\n","print(f\"âœ… Total chunks: {len(chunked_docs)}\")"],"metadata":{"id":"pgZBdFg9gjhg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 2: Base de Datos Vectorial"],"metadata":{"id":"xo3Q39OwgsAp"}},{"cell_type":"markdown","source":[],"metadata":{"id":"e472kRsbgs0K"}},{"cell_type":"markdown","source":["### 5. CreaciÃ³n de Embeddings y ChromaDB"],"metadata":{"id":"sVBDE5W8guio"}},{"cell_type":"code","source":["class VectorDatabase:\n","    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n","        self.embeddings = HuggingFaceEmbeddings(\n","            model_name=model_name,\n","            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n","        )\n","        self.vectorstore = None\n","\n","    def create_vectorstore(self, documents: List[Document], persist_directory: str = \"./chroma_db\"):\n","        \"\"\"Crea base de datos vectorial con ChromaDB\"\"\"\n","        print(\"ğŸ”„ Creando embeddings y base vectorial...\")\n","\n","        self.vectorstore = Chroma.from_documents(\n","            documents=documents,\n","            embedding=self.embeddings,\n","            persist_directory=persist_directory\n","        )\n","\n","        print(f\"âœ… Base vectorial creada con {len(documents)} documentos\")\n","        return self.vectorstore\n","\n","    def similarity_search(self, query: str, k: int = 5) -> List[Document]:\n","        \"\"\"BÃºsqueda por similitud\"\"\"\n","        if not self.vectorstore:\n","            raise ValueError(\"Vectorstore no inicializado\")\n","\n","        return self.vectorstore.similarity_search(query, k=k)"],"metadata":{"id":"jdRok0pTgvqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear base vectorial\n","vector_db = VectorDatabase()\n","vectorstore = vector_db.create_vectorstore(chunked_docs)"],"metadata":{"id":"2I-VaqGBg7kR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 3: Sistema LLM y RAG"],"metadata":{"id":"gP8YHmQAhY7J"}},{"cell_type":"markdown","source":["### 6. ConfiguraciÃ³n del Modelo de Lenguaje"],"metadata":{"id":"dRy59EBdhaUY"}},{"cell_type":"code","source":["class LLMManager:\n","    def __init__(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n","        \"\"\"Usa modelo mÃ¡s ligero para Colab gratuito\"\"\"\n","        self.model_name = model_name\n","        self.tokenizer = None\n","        self.model = None\n","        self.pipeline = None\n","        self.setup_model()\n","\n","    def setup_model(self):\n","        \"\"\"Configura el modelo de lenguaje\"\"\"\n","        print(f\"ğŸ”„ Cargando modelo {self.model_name}...\")\n","\n","        # Para Colab gratuito, usar pipeline mÃ¡s eficiente\n","        self.pipeline = pipeline(\n","            \"text-generation\",\n","            model=self.model_name,\n","            tokenizer=self.model_name,\n","            device=0 if torch.cuda.is_available() else -1,\n","            max_length=512,\n","            do_sample=True,\n","            temperature=0.7\n","        )\n","\n","        print(\"âœ… Modelo cargado correctamente\")\n","\n","    def generate_response(self, prompt: str, max_length: int = 300) -> str:\n","        \"\"\"Genera respuesta usando el LLM\"\"\"\n","        if not self.pipeline:\n","            raise ValueError(\"Pipeline no inicializado\")\n","\n","        # Generar respuesta\n","        response = self.pipeline(\n","            prompt,\n","            max_length=max_length,\n","            num_return_sequences=1,\n","            pad_token_id=self.pipeline.tokenizer.eos_token_id\n","        )\n","\n","        # Extraer solo la parte nueva generada\n","        generated_text = response[0]['generated_text']\n","        new_text = generated_text[len(prompt):].strip()\n","\n","        return new_text\n"],"metadata":{"id":"5RlXYTvdheZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inicializar LLM\n","llm_manager = LLMManager()"],"metadata":{"id":"dqM2oWzvhuXK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7. Sistema RAG BÃ¡sico"],"metadata":{"id":"qASFF3mWiuNj"}},{"cell_type":"code","source":["class RAGSystem:\n","    def __init__(self, vector_db: VectorDatabase, llm_manager: LLMManager):\n","        self.vector_db = vector_db\n","        self.llm_manager = llm_manager\n","        self.tortured_phrases = {\n","            \"bosom malignancy\": \"breast cancer\",\n","            \"fake patterns\": \"actual patterns\",\n","            \"artificial results\": \"research results\"\n","        }\n","\n","    def retrieve_context(self, query: str, k: int = 3) -> List[Document]:\n","        \"\"\"Recupera contexto relevante\"\"\"\n","        return self.vector_db.similarity_search(query, k=k)\n","\n","    def create_prompt(self, query: str, context_docs: List[Document]) -> str:\n","        \"\"\"Crea prompt con contexto\"\"\"\n","        context = \"\\n\\n\".join([\n","            f\"Document {i+1}: {doc.page_content}\"\n","            for i, doc in enumerate(context_docs)\n","        ])\n","\n","        prompt = f\"\"\"Based on the following scientific documents, please answer the question accurately.\n","\n","Context:\n","{context}\n","\n","Question: {query}\n","\n","Answer:\"\"\"\n","\n","        return prompt\n","\n","    def generate_rag_response(self, query: str) -> Dict[str, Any]:\n","        \"\"\"Genera respuesta completa RAG\"\"\"\n","        # 1. Recuperar contexto\n","        context_docs = self.retrieve_context(query)\n","\n","        # 2. Crear prompt\n","        prompt = self.create_prompt(query, context_docs)\n","\n","        # 3. Generar respuesta\n","        response = self.llm_manager.generate_response(prompt)\n","\n","        # 4. Preparar resultado\n","        result = {\n","            'query': query,\n","            'response': response,\n","            'context_docs': context_docs,\n","            'sources': [doc.metadata for doc in context_docs],\n","            'prompt_used': prompt\n","        }\n","\n","        return result\n","\n","# Inicializar sistema RAG\n","rag_system = RAGSystem(vector_db, llm_manager)\n"],"metadata":{"id":"Aa46jJg5isE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 4: Componente XAI (Explicabilidad)"],"metadata":{"id":"DTfHydgIi9sV"}},{"cell_type":"markdown","source":["### 8. Detector de Hallucinations y Tortured Phrases"],"metadata":{"id":"dVPCVN98jAlC"}},{"cell_type":"code","source":["class FactChecker:\n","    def __init__(self):\n","        self.tortured_phrases = {\n","            \"bosom malignancy\": {\n","                \"correct\": \"breast cancer\",\n","                \"confidence\": 0.95,\n","                \"explanation\": \"TÃ©rmino mÃ©dico no estÃ¡ndar que puede indicar contenido generado automÃ¡ticamente\"\n","            },\n","            \"fake patterns\": {\n","                \"correct\": \"patterns\",\n","                \"confidence\": 0.85,\n","                \"explanation\": \"Uso de 'fake' puede indicar contenido poco confiable\"\n","            }\n","        }\n","\n","        self.medical_terms_validation = {\n","            \"breast cancer\": True,\n","            \"machine learning\": True,\n","            \"bosom malignancy\": False\n","        }\n","\n","    def detect_tortured_phrases(self, text: str) -> List[Dict]:\n","        \"\"\"Detecta frases problemÃ¡ticas\"\"\"\n","        detections = []\n","\n","        for phrase, info in self.tortured_phrases.items():\n","            if phrase.lower() in text.lower():\n","                detection = {\n","                    'phrase': phrase,\n","                    'position': text.lower().index(phrase.lower()),\n","                    'severity': 'high' if info['confidence'] > 0.9 else 'medium',\n","                    'suggested_correction': info['correct'],\n","                    'explanation': info['explanation'],\n","                    'confidence': info['confidence']\n","                }\n","                detections.append(detection)\n","\n","        return detections\n","\n","    def validate_against_sources(self, response: str, source_docs: List[Document]) -> Dict:\n","        \"\"\"Valida respuesta contra fuentes\"\"\"\n","        source_text = \" \".join([doc.page_content for doc in source_docs])\n","\n","        # AnÃ¡lisis simple de overlap\n","        response_words = set(response.lower().split())\n","        source_words = set(source_text.lower().split())\n","\n","        overlap = response_words.intersection(source_words)\n","        overlap_ratio = len(overlap) / len(response_words) if response_words else 0\n","\n","        validation = {\n","            'overlap_ratio': overlap_ratio,\n","            'supported': overlap_ratio > 0.3,  # Umbral simple\n","            'unsupported_words': response_words - source_words,\n","            'explanation': f\"La respuesta tiene {overlap_ratio:.2%} de palabras respaldadas por las fuentes\"\n","        }\n","\n","        return validation\n","\n","    def comprehensive_check(self, response: str, source_docs: List[Document]) -> Dict:\n","        \"\"\"VerificaciÃ³n completa\"\"\"\n","        # Detectar tortured phrases\n","        tortured_detections = self.detect_tortured_phrases(response)\n","\n","        # Validar contra fuentes\n","        source_validation = self.validate_against_sources(response, source_docs)\n","\n","        # Calcular score de confiabilidad\n","        reliability_score = 1.0\n","\n","        # Penalizar por tortured phrases\n","        for detection in tortured_detections:\n","            penalty = detection['confidence'] * 0.3\n","            reliability_score -= penalty\n","\n","        # Penalizar por bajo respaldo de fuentes\n","        if not source_validation['supported']:\n","            reliability_score -= 0.4\n","\n","        reliability_score = max(0.0, reliability_score)\n","\n","        return {\n","            'tortured_phrases': tortured_detections,\n","            'source_validation': source_validation,\n","            'reliability_score': reliability_score,\n","            'is_reliable': reliability_score > 0.6,\n","            'warnings': self._generate_warnings(tortured_detections, source_validation)\n","        }\n","\n","    def _generate_warnings(self, tortured_detections: List, source_validation: Dict) -> List[str]:\n","        \"\"\"Genera advertencias para el usuario\"\"\"\n","        warnings = []\n","\n","        if tortured_detections:\n","            warnings.append(f\"âš ï¸ Detectadas {len(tortured_detections)} frases problemÃ¡ticas\")\n","\n","        if not source_validation['supported']:\n","            warnings.append(\"âš ï¸ Respuesta poco respaldada por fuentes originales\")\n","\n","        return warnings\n","\n","# Inicializar fact checker\n","fact_checker = FactChecker()"],"metadata":{"id":"AzU6dF_ajDiR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9. Sistema de Explicabilidad con SHAP/LIME\n"],"metadata":{"id":"ylfLAPtsjRlp"}},{"cell_type":"code","source":["class ExplainabilityEngine:\n","    def __init__(self, rag_system: RAGSystem):\n","        self.rag_system = rag_system\n","        self.lime_explainer = LimeTextExplainer(class_names=['reliable', 'unreliable'])\n","\n","    def explain_response_sources(self, query: str, response: str, source_docs: List[Document]) -> Dict:\n","        \"\"\"Explica quÃ© fuentes contribuyeron a la respuesta\"\"\"\n","        explanations = []\n","\n","        for i, doc in enumerate(source_docs):\n","            # Calcular similitud simple entre respuesta y documento\n","            doc_words = set(doc.page_content.lower().split())\n","            response_words = set(response.lower().split())\n","\n","            common_words = doc_words.intersection(response_words)\n","            influence_score = len(common_words) / len(response_words) if response_words else 0\n","\n","            explanation = {\n","                'source_id': i,\n","                'title': doc.metadata.get('title', f'Document {i+1}'),\n","                'influence_score': influence_score,\n","                'common_concepts': list(common_words)[:10],  # Top 10\n","                'metadata': doc.metadata\n","            }\n","            explanations.append(explanation)\n","\n","        # Ordenar por influencia\n","        explanations.sort(key=lambda x: x['influence_score'], reverse=True)\n","\n","        return {\n","            'source_influences': explanations,\n","            'most_influential': explanations[0] if explanations else None,\n","            'explanation_summary': self._create_influence_summary(explanations)\n","        }\n","\n","    def explain_reliability_factors(self, fact_check_result: Dict) -> Dict:\n","        \"\"\"Explica factores que afectan la confiabilidad\"\"\"\n","        factors = []\n","\n","        # Factor: Tortured phrases\n","        if fact_check_result['tortured_phrases']:\n","            factor = {\n","                'type': 'tortured_phrases',\n","                'impact': 'negative',\n","                'weight': 0.3,\n","                'description': f\"Detectadas {len(fact_check_result['tortured_phrases'])} frases problemÃ¡ticas\",\n","                'details': fact_check_result['tortured_phrases']\n","            }\n","            factors.append(factor)\n","\n","        # Factor: Source support\n","        source_support = fact_check_result['source_validation']['supported']\n","        factor = {\n","            'type': 'source_support',\n","            'impact': 'positive' if source_support else 'negative',\n","            'weight': 0.4,\n","            'description': f\"Respaldo de fuentes: {'Alto' if source_support else 'Bajo'}\",\n","            'details': fact_check_result['source_validation']\n","        }\n","        factors.append(factor)\n","\n","        return {\n","            'reliability_factors': factors,\n","            'overall_score': fact_check_result['reliability_score'],\n","            'recommendation': self._get_reliability_recommendation(fact_check_result['reliability_score'])\n","        }\n","\n","    def _create_influence_summary(self, explanations: List[Dict]) -> str:\n","        \"\"\"Crea resumen de influencias\"\"\"\n","        if not explanations:\n","            return \"No se encontraron fuentes influyentes\"\n","\n","        top_source = explanations[0]\n","        return f\"La fuente mÃ¡s influyente es '{top_source['title']}' con {top_source['influence_score']:.2%} de influencia\"\n","\n","    def _get_reliability_recommendation(self, score: float) -> str:\n","        \"\"\"Obtiene recomendaciÃ³n basada en score\"\"\"\n","        if score > 0.8:\n","            return \"âœ… Alta confiabilidad - InformaciÃ³n respaldada por fuentes\"\n","        elif score > 0.6:\n","            return \"âš ï¸ Confiabilidad moderada - Verificar informaciÃ³n importante\"\n","        else:\n","            return \"âŒ Baja confiabilidad - Consultar fuentes adicionales\"\n","\n","# Inicializar motor de explicabilidad\n","explainability_engine = ExplainabilityEngine(rag_system)"],"metadata":{"id":"MrByOmyejSgB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 5: Sistema Integrado XAI-RAG"],"metadata":{"id":"uOw2g2pmjd8C"}},{"cell_type":"markdown","source":["### 10. Sistema Principal"],"metadata":{"id":"q-kTgMvbjggC"}},{"cell_type":"code","source":["class FactCheckXAIRAG:\n","    def __init__(self, rag_system: RAGSystem, fact_checker: FactChecker,\n","                 explainability_engine: ExplainabilityEngine):\n","        self.rag_system = rag_system\n","        self.fact_checker = fact_checker\n","        self.explainability_engine = explainability_engine\n","\n","    def process_query(self, query: str) -> Dict[str, Any]:\n","        \"\"\"Procesa query completo con XAI\"\"\"\n","        print(f\"ğŸ”„ Procesando: {query}\")\n","\n","        # 1. Generar respuesta RAG\n","        rag_result = self.rag_system.generate_rag_response(query)\n","\n","        # 2. Verificar hechos\n","        fact_check_result = self.fact_checker.comprehensive_check(\n","            rag_result['response'],\n","            rag_result['context_docs']\n","        )\n","\n","        # 3. Generar explicaciones\n","        source_explanation = self.explainability_engine.explain_response_sources(\n","            query,\n","            rag_result['response'],\n","            rag_result['context_docs']\n","        )\n","\n","        reliability_explanation = self.explainability_engine.explain_reliability_factors(\n","            fact_check_result\n","        )\n","\n","        # 4. Compilar resultado completo\n","        complete_result = {\n","            'query': query,\n","            'response': rag_result['response'],\n","            'sources': rag_result['sources'],\n","            'fact_check': fact_check_result,\n","            'explanations': {\n","                'source_influence': source_explanation,\n","                'reliability_factors': reliability_explanation\n","            },\n","            'metadata': {\n","                'timestamp': datetime.now().isoformat(),\n","                'model_used': self.rag_system.llm_manager.model_name,\n","                'total_sources': len(rag_result['context_docs'])\n","            }\n","        }\n","\n","        return complete_result\n","\n","    def format_response_for_display(self, result: Dict) -> str:\n","        \"\"\"Formatea respuesta para mostrar al usuario\"\"\"\n","        output = []\n","\n","        # Respuesta principal\n","        output.append(f\"**Respuesta:**\\n{result['response']}\\n\")\n","\n","        # Advertencias si las hay\n","        warnings = result['fact_check']['warnings']\n","        if warnings:\n","            output.append(\"**âš ï¸ Advertencias:**\")\n","            for warning in warnings:\n","                output.append(f\"- {warning}\")\n","            output.append(\"\")\n","\n","        # Score de confiabilidad\n","        reliability = result['fact_check']['reliability_score']\n","        recommendation = result['explanations']['reliability_factors']['recommendation']\n","        output.append(f\"**Confiabilidad:** {reliability:.2%}\")\n","        output.append(f\"{recommendation}\\n\")\n","\n","        # Fuentes mÃ¡s influyentes\n","        most_influential = result['explanations']['source_influence']['most_influential']\n","        if most_influential:\n","            output.append(f\"**Fuente principal:** {most_influential['title']}\")\n","            output.append(f\"**Influencia:** {most_influential['influence_score']:.2%}\\n\")\n","\n","        # Tortured phrases detectadas\n","        tortured = result['fact_check']['tortured_phrases']\n","        if tortured:\n","            output.append(\"**ğŸ” Frases problemÃ¡ticas detectadas:**\")\n","            for detection in tortured:\n","                output.append(f\"- '{detection['phrase']}' â†’ Sugerencia: '{detection['suggested_correction']}'\")\n","                output.append(f\"  {detection['explanation']}\")\n","            output.append(\"\")\n","\n","        return \"\\n\".join(output)\n","\n","# Inicializar sistema completo\n","xai_rag_system = FactCheckXAIRAG(rag_system, fact_checker, explainability_engine)"],"metadata":{"id":"4GCa-gmjjiiL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 6: Interfaz de Usuario con Gradio"],"metadata":{"id":"jJOmHccMmB_t"}},{"cell_type":"markdown","source":["### 11. Interfaz Web Interactiva"],"metadata":{"id":"P6KX-KilmD-b"}},{"cell_type":"code","source":["def create_gradio_interface():\n","    \"\"\"Crea interfaz Gradio para el sistema\"\"\"\n","\n","    def process_user_query(query, show_detailed_analysis):\n","        \"\"\"Procesa query del usuario\"\"\"\n","        if not query.strip():\n","            return \"Por favor, ingresa una pregunta.\", \"\"\n","\n","        try:\n","            # Procesar con el sistema XAI-RAG\n","            result = xai_rag_system.process_query(query)\n","\n","            # Respuesta formateada para mostrar\n","            formatted_response = xai_rag_system.format_response_for_display(result)\n","\n","            # AnÃ¡lisis detallado si se solicita\n","            detailed_analysis = \"\"\n","            if show_detailed_analysis:\n","                detailed_analysis = format_detailed_analysis(result)\n","\n","            return formatted_response, detailed_analysis\n","\n","        except Exception as e:\n","            return f\"âŒ Error procesando la consulta: {str(e)}\", \"\"\n","\n","    def format_detailed_analysis(result):\n","        \"\"\"Formatea anÃ¡lisis detallado\"\"\"\n","        analysis = []\n","\n","        analysis.append(\"## ğŸ“Š AnÃ¡lisis Detallado\\n\")\n","\n","        # InformaciÃ³n de fuentes\n","        analysis.append(\"### ğŸ“š Fuentes Utilizadas:\")\n","        for i, source in enumerate(result['sources']):\n","            analysis.append(f\"{i+1}. **{source.get('title', 'Sin tÃ­tulo')}**\")\n","            if 'authors' in source:\n","                analysis.append(f\"   - Autores: {source['authors']}\")\n","            if 'published' in source:\n","                analysis.append(f\"   - Publicado: {source['published']}\")\n","            analysis.append(\"\")\n","\n","        # Influencia de fuentes\n","        source_influences = result['explanations']['source_influence']['source_influences']\n","        analysis.append(\"### ğŸ¯ Influencia de Fuentes:\")\n","        for influence in source_influences[:3]:  # Top 3\n","            analysis.append(f\"- **{influence['title']}**: {influence['influence_score']:.2%} influencia\")\n","\n","        # Factores de confiabilidad\n","        analysis.append(\"\\n### ğŸ” Factores de Confiabilidad:\")\n","        reliability_factors = result['explanations']['reliability_factors']['reliability_factors']\n","        for factor in reliability_factors:\n","            icon = \"âœ…\" if factor['impact'] == 'positive' else \"âŒ\"\n","            analysis.append(f\"{icon} **{factor['type']}**: {factor['description']}\")\n","\n","        return \"\\n\".join(analysis)\n","\n","    # Crear interfaz\n","    with gr.Blocks(title=\"FactCheck XAI-RAG\", theme=gr.themes.Soft()) as interface:\n","        gr.Markdown(\"\"\"\n","        # ğŸ”¬ FactCheck XAI-RAG\n","        ### Sistema de VerificaciÃ³n de Hechos Explicable para Preguntas CientÃ­ficas\n","\n","        Este sistema utiliza RAG (Retrieval-Augmented Generation) con explicabilidad (XAI) para:\n","        - âœ… Responder preguntas cientÃ­ficas con fuentes\n","        - ğŸ” Detectar informaciÃ³n potencialmente incorrecta\n","        - ğŸ“Š Explicar el proceso de generaciÃ³n de respuestas\n","        - âš ï¸ Alertar sobre contenido poco confiable\n","        \"\"\")\n","\n","        with gr.Row():\n","            with gr.Column(scale=2):\n","                query_input = gr.Textbox(\n","                    label=\"ğŸ’¬ Tu pregunta cientÃ­fica\",\n","                    placeholder=\"Ej: Â¿QuÃ© avances recientes hay en machine learning para detecciÃ³n de cÃ¡ncer?\",\n","                    lines=3\n","                )\n","\n","                show_detailed = gr.Checkbox(\n","                    label=\"ğŸ“Š Mostrar anÃ¡lisis detallado\",\n","                    value=False\n","                )\n","\n","                submit_btn = gr.Button(\"ğŸ” Analizar\", variant=\"primary\")\n","\n","            with gr.Column(scale=1):\n","                gr.Markdown(\"\"\"\n","                **ğŸ’¡ Ejemplos de preguntas:**\n","                - \"Â¿QuÃ© es la bosom malignancy?\"\n","                - \"Â¿CÃ³mo funciona machine learning en medicina?\"\n","                - \"Â¿CuÃ¡les son los tratamientos para breast cancer?\"\n","                \"\"\")\n","\n","        with gr.Row():\n","            with gr.Column():\n","                response_output = gr.Markdown(label=\"ğŸ“‹ Respuesta y AnÃ¡lisis\")\n","\n","            with gr.Column():\n","                detailed_output = gr.Markdown(label=\"ğŸ“Š AnÃ¡lisis Detallado\", visible=False)\n","\n","        # Mostrar/ocultar anÃ¡lisis detallado\n","        def toggle_detailed(show_detailed):\n","            return gr.update(visible=show_detailed)\n","\n","        show_detailed.change(\n","            fn=toggle_detailed,\n","            inputs=[show_detailed],\n","            outputs=[detailed_output]\n","        )\n","\n","        # Procesar consulta\n","        submit_btn.click(\n","            fn=process_user_query,\n","            inputs=[query_input, show_detailed],\n","            outputs=[response_output, detailed_output]\n","        )\n","\n","        # Ejemplos interactivos\n","        gr.Examples(\n","            examples=[\n","                [\"Â¿QuÃ© avances hay en machine learning para detecciÃ³n de bosom malignancy?\", True],\n","                [\"Â¿CÃ³mo funciona la inteligencia artificial en medicina?\", False],\n","                [\"Â¿CuÃ¡les son los sÃ­ntomas del breast cancer?\", True]\n","            ],\n","            inputs=[query_input, show_detailed]\n","        )\n","\n","    return interface\n"],"metadata":{"id":"knW8zdb7mAC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear y lanzar interfaz\n","print(\"ğŸš€ Creando interfaz web...\")\n","interface = create_gradio_interface()"],"metadata":{"id":"pVclctNYmYZE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 7: Lanzamiento y Pruebas"],"metadata":{"id":"BH2dzTp3mBE8"}},{"cell_type":"markdown","source":["### 12. ConfiguraciÃ³n Final y Lanzamiento"],"metadata":{"id":"zdFf4w_RnRq3"}},{"cell_type":"code","source":["# Configurar ngrok para acceso pÃºblico (opcional)\n","def setup_public_access():\n","    \"\"\"Configura acceso pÃºblico con ngrok\"\"\"\n","    try:\n","        # Configurar token de ngrok si tienes uno\n","        # ngrok.set_auth_token(\"tu_token_aqui\")\n","\n","        # Lanzar tÃºnel\n","        public_url = ngrok.connect(7860)\n","        print(f\"ğŸŒ URL pÃºblica: {public_url}\")\n","        return public_url\n","    except Exception as e:\n","        print(f\"âš ï¸ No se pudo configurar acceso pÃºblico: {e}\")\n","        return None\n"],"metadata":{"id":"pFZXVYqgnT99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FunciÃ³n principal de lanzamiento\n","def launch_system():\n","    \"\"\"Lanza el sistema completo\"\"\"\n","    print(\"ğŸš€ Iniciando FactCheck XAI-RAG...\")\n","\n","    # Verificar que todo estÃ© inicializado\n","    components_status = {\n","        'Vector Database': vectorstore is not None,\n","        'LLM Manager': llm_manager.pipeline is not None,\n","        'RAG System': rag_system is not None,\n","        'Fact Checker': fact_checker is not None,\n","        'XAI Engine': explainability_engine is not None,\n","        'Main System': xai_rag_system is not None\n","    }\n","\n","    print(\"\\nğŸ“‹ Estado de componentes:\")\n","    for component, status in components_status.items():\n","        status_icon = \"âœ…\" if status else \"âŒ\"\n","        print(f\"{status_icon} {component}\")\n","\n","    if not all(components_status.values()):\n","        print(\"âŒ Error: Algunos componentes no estÃ¡n inicializados\")\n","        return False\n","\n","    # Configurar acceso pÃºblico (opcional)\n","    # public_url = setup_public_access()\n","\n","    # Lanzar interfaz\n","    print(\"\\nğŸŒ Lanzando interfaz web...\")\n","    interface.launch(\n","        server_name=\"0.0.0.0\",\n","        server_port=7860,\n","        share=True,  # Esto crea un enlace pÃºblico temporal\n","        debug=True\n","    )\n","\n","    return True\n"],"metadata":{"id":"XGdCzGH-ns0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FunciÃ³n de pruebas rÃ¡pidas\n","def run_quick_tests():\n","    \"\"\"Ejecuta pruebas rÃ¡pidas del sistema\"\"\"\n","    print(\"ğŸ§ª Ejecutando pruebas rÃ¡pidas...\\n\")\n","\n","    test_queries = [\n","        \"Â¿QuÃ© es la bosom malignancy?\",  # DeberÃ­a detectar tortured phrase\n","        \"Â¿CÃ³mo funciona machine learning en medicina?\",  # Pregunta normal\n","        \"Â¿CuÃ¡les son los tratamientos para breast cancer?\"  # Pregunta mÃ©dica vÃ¡lida\n","    ]\n","\n","    for i, query in enumerate(test_queries, 1):\n","        print(f\"ğŸ”¬ Prueba {i}: {query}\")\n","        try:\n","            result = xai_rag_system.process_query(query)\n","\n","            # Mostrar resultados clave\n","            print(f\"   ğŸ“ Respuesta: {result['response'][:100]}...\")\n","            print(f\"   ğŸ¯ Confiabilidad: {result['fact_check']['reliability_score']:.2%}\")\n","            print(f\"   âš ï¸ Advertencias: {len(result['fact_check']['warnings'])}\")\n","            print(f\"   ğŸ“š Fuentes: {len(result['sources'])}\")\n","\n","            if result['fact_check']['tortured_phrases']:\n","                print(f\"   ğŸ” Tortured phrases: {len(result['fact_check']['tortured_phrases'])}\")\n","\n","            print()\n","\n","        except Exception as e:\n","            print(f\"   âŒ Error: {e}\\n\")\n","\n","    print(\"âœ… Pruebas completadas\")\n"],"metadata":{"id":"QlzmWCXtn_1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ejecutar pruebas primero\n","run_quick_tests()"],"metadata":{"id":"8nrFH_tQoIhF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lanzar sistema\n","if launch_system():\n","    print(\"\"\"\n","    ğŸ‰ Â¡Sistema FactCheck XAI-RAG lanzado exitosamente!\n","\n","    ğŸ“± Usa la interfaz web para:\n","    - Hacer preguntas cientÃ­ficas\n","    - Ver explicaciones detalladas\n","    - Identificar informaciÃ³n poco confiable\n","\n","    ğŸ”¬ CaracterÃ­sticas principales activas:\n","    âœ… DetecciÃ³n de tortured phrases\n","    âœ… VerificaciÃ³n contra fuentes\n","    âœ… Explicabilidad de respuestas\n","    âœ… Scoring de confiabilidad\n","    âœ… Interfaz intuitiva\n","    \"\"\")\n","else:\n","    print(\"âŒ Error al lanzar el sistema\")\n"],"metadata":{"id":"hMyGyM0KoSLn"},"execution_count":null,"outputs":[]}]}