{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZA7YwN6fg+jUqSvKOvbpr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["### 1. Configuración Inicial de Colab"],"metadata":{"id":"tzvTv3RlaX5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"E5UAFQr0RP4B"},"outputs":[],"source":["# 1. Instalar dependencias principales\n","!pip install transformers torch accelerate\n","!pip install langchain langchain-community\n","!pip install chromadb sentence-transformers\n","!pip install fastapi uvicorn pyngrok\n","!pip install gradio pandas numpy\n","!pip install shap lime-tabular\n","!pip install arxiv requests beautifulsoup4"]},{"cell_type":"code","source":["### 2. Importaciones y Configuración"],"metadata":{"id":"OnE7OAq3abMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from typing import List, Dict, Any, Optional\n","import json\n","import re\n","from datetime import datetime"],"metadata":{"id":"YFNDUXBe9A8l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LangChain y RAG\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.document_loaders import TextLoader\n","from langchain.schema import Document"],"metadata":{"id":"SmJo_3FE-iMh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So far so good. Main Dependencies were installed Ok."],"metadata":{"id":"EY0I2-cTTnTZ"}},{"cell_type":"code","source":["# Transformers para LLM\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","import torch"],"metadata":{"id":"_1LOITna-xys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explicabilidad\n","!pip install shap\n","!pip install lime\n","\n","import shap\n","from lime.lime_text import LimeTextExplainer"],"metadata":{"collapsed":true,"id":"nXvzd2NAYvI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# API y Frontend\n","import gradio as gr\n","from pyngrok import ngrok"],"metadata":{"id":"UMxlj8bSZ4f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Utilidades\n","import requests\n","from bs4 import BeautifulSoup\n","import arxiv\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"SeQD2iXfZ92m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 1: Preparación de Datos Científicos"],"metadata":{"id":"BvwkClKhagix"}},{"cell_type":"markdown","source":["### 3. Descarga de Corpus Científico"],"metadata":{"id":"CMDqAgrLafOV"}},{"cell_type":"code","source":["class ScientificDataCollector:\n","    def __init__(self):\n","        self.documents = []\n","        self.tortured_phrases = {\n","            \"bosom malignancy\": \"breast cancer\",\n","            \"corpus luteum\": \"corpus luteum\",  # correcto\n","            \"fake example\": \"real term\"\n","        }\n","\n","    def collect_arxiv_papers(self, query: str, max_results: int = 20):\n","        \"\"\"Descarga papers de ArXiv en biomedicina\"\"\"\n","        client = arxiv.Client()\n","        search = arxiv.Search(\n","            query=f\"{query} AND cat:q-bio*\",\n","            max_results=max_results,\n","            sort_by=arxiv.SortCriterion.Relevance\n","        )\n","\n","        papers = []\n","        for result in client.results(search):\n","            paper = {\n","                'title': result.title,\n","                'authors': [author.name for author in result.authors],\n","                'abstract': result.summary,\n","                'url': result.pdf_url,\n","                'published': result.published.strftime('%Y-%m-%d'),\n","                'categories': result.categories\n","            }\n","            papers.append(paper)\n","\n","        return papers\n","\n","    def create_synthetic_contaminated_data(self):\n","        \"\"\"Crea datos sintéticos con 'tortured phrases'\"\"\"\n","        contaminated_texts = [\n","            \"The study of bosom malignancy has shown significant progress in recent years. Traditional treatments for bosom malignancy include chemotherapy and radiation.\",\n","            \"Machine learning models can help detect fake patterns in medical imaging, improving diagnostic accuracy.\",\n","            \"Recent research in artificial intelligence applications for medical diagnosis shows promising results.\"\n","        ]\n","\n","        clean_texts = [\n","            \"Breast cancer research has advanced significantly with new immunotherapy approaches. Clinical trials show improved survival rates.\",\n","            \"Deep learning algorithms demonstrate high accuracy in medical image analysis for early disease detection.\",\n","            \"Natural language processing techniques are being applied to clinical notes for better patient care.\"\n","        ]\n","\n","        return contaminated_texts + clean_texts\n"],"metadata":{"id":"ufGJxPKIaRId"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Recolectar datos\n","collector = ScientificDataCollector()"],"metadata":{"id":"V-ZOcdbvafsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datos de ArXiv (biomedicina)\n","print(\"🔄 Descargando papers de ArXiv...\")\n","arxiv_papers = collector.collect_arxiv_papers(\"cancer detection machine learning\", 15)"],"metadata":{"id":"vfr4P_eodwUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datos sintéticos contaminados\n","synthetic_data = collector.create_synthetic_contaminated_data()"],"metadata":{"id":"fzb6eMa5egDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"✅ Recolectados {len(arxiv_papers)} papers de ArXiv\")\n","print(f\"✅ Creados {len(synthetic_data)} documentos sintéticos\")"],"metadata":{"id":"Z3JutZk9e1S2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 4. Procesamiento y Chunking\n","\n","class DocumentPreprocessor:\n","    def __init__(self):\n","        self.text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=500,\n","            chunk_overlap=50,\n","            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n","        )\n","\n","    def process_arxiv_papers(self, papers: List[Dict]) -> List[Document]:\n","        \"\"\"Convierte papers de ArXiv a documentos LangChain\"\"\"\n","        documents = []\n","\n","        for paper in papers:\n","            # Combinar título y abstract\n","            content = f\"Title: {paper['title']}\\n\\nAbstract: {paper['abstract']}\"\n","\n","            # Crear documento con metadata\n","            doc = Document(\n","                page_content=content,\n","                metadata={\n","                    'source': 'arxiv',\n","                    'title': paper['title'],\n","                    'authors': ', '.join(paper['authors']),\n","                    'url': paper['url'],\n","                    'published': paper['published'],\n","                    'categories': ', '.join(paper['categories'])\n","                }\n","            )\n","            documents.append(doc)\n","\n","        return documents\n","\n","    def process_synthetic_data(self, texts: List[str]) -> List[Document]:\n","        \"\"\"Procesa datos sintéticos\"\"\"\n","        documents = []\n","\n","        for i, text in enumerate(texts):\n","            doc = Document(\n","                page_content=text,\n","                metadata={\n","                    'source': 'synthetic',\n","                    'doc_id': f'synthetic_{i}',\n","                    'contaminated': 'bosom malignancy' in text or 'fake' in text\n","                }\n","            )\n","            documents.append(doc)\n","\n","        return documents\n","\n","    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n","        \"\"\"Divide documentos en chunks\"\"\"\n","        return self.text_splitter.split_documents(documents)\n"],"metadata":{"id":"OCXcc_lSfAcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Procesar documentos\n","preprocessor = DocumentPreprocessor()"],"metadata":{"id":"bXzVFkuQgTMR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"🔄 Procesando documentos...\")\n","arxiv_docs = preprocessor.process_arxiv_papers(arxiv_papers)\n","synthetic_docs = preprocessor.process_synthetic_data(synthetic_data)"],"metadata":{"id":"ns77F75Ugcb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_documents = arxiv_docs + synthetic_docs\n","chunked_docs = preprocessor.chunk_documents(all_documents)\n","\n","print(f\"✅ Total documentos: {len(all_documents)}\")\n","print(f\"✅ Total chunks: {len(chunked_docs)}\")"],"metadata":{"id":"pgZBdFg9gjhg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 2: Base de Datos Vectorial"],"metadata":{"id":"xo3Q39OwgsAp"}},{"cell_type":"markdown","source":[],"metadata":{"id":"e472kRsbgs0K"}},{"cell_type":"markdown","source":["### 5. Creación de Embeddings y ChromaDB"],"metadata":{"id":"sVBDE5W8guio"}},{"cell_type":"code","source":["class VectorDatabase:\n","    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n","        self.embeddings = HuggingFaceEmbeddings(\n","            model_name=model_name,\n","            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n","        )\n","        self.vectorstore = None\n","\n","    def create_vectorstore(self, documents: List[Document], persist_directory: str = \"./chroma_db\"):\n","        \"\"\"Crea base de datos vectorial con ChromaDB\"\"\"\n","        print(\"🔄 Creando embeddings y base vectorial...\")\n","\n","        self.vectorstore = Chroma.from_documents(\n","            documents=documents,\n","            embedding=self.embeddings,\n","            persist_directory=persist_directory\n","        )\n","\n","        print(f\"✅ Base vectorial creada con {len(documents)} documentos\")\n","        return self.vectorstore\n","\n","    def similarity_search(self, query: str, k: int = 5) -> List[Document]:\n","        \"\"\"Búsqueda por similitud\"\"\"\n","        if not self.vectorstore:\n","            raise ValueError(\"Vectorstore no inicializado\")\n","\n","        return self.vectorstore.similarity_search(query, k=k)"],"metadata":{"id":"jdRok0pTgvqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear base vectorial\n","vector_db = VectorDatabase()\n","vectorstore = vector_db.create_vectorstore(chunked_docs)"],"metadata":{"id":"2I-VaqGBg7kR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 3: Sistema LLM y RAG"],"metadata":{"id":"gP8YHmQAhY7J"}},{"cell_type":"markdown","source":["### 6. Configuración del Modelo de Lenguaje"],"metadata":{"id":"dRy59EBdhaUY"}},{"cell_type":"code","source":["class LLMManager:\n","    def __init__(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n","        \"\"\"Usa modelo más ligero para Colab gratuito\"\"\"\n","        self.model_name = model_name\n","        self.tokenizer = None\n","        self.model = None\n","        self.pipeline = None\n","        self.setup_model()\n","\n","    def setup_model(self):\n","        \"\"\"Configura el modelo de lenguaje\"\"\"\n","        print(f\"🔄 Cargando modelo {self.model_name}...\")\n","\n","        # Para Colab gratuito, usar pipeline más eficiente\n","        self.pipeline = pipeline(\n","            \"text-generation\",\n","            model=self.model_name,\n","            tokenizer=self.model_name,\n","            device=0 if torch.cuda.is_available() else -1,\n","            max_length=512,\n","            do_sample=True,\n","            temperature=0.7\n","        )\n","\n","        print(\"✅ Modelo cargado correctamente\")\n","\n","    def generate_response(self, prompt: str, max_length: int = 300) -> str:\n","        \"\"\"Genera respuesta usando el LLM\"\"\"\n","        if not self.pipeline:\n","            raise ValueError(\"Pipeline no inicializado\")\n","\n","        # Generar respuesta\n","        response = self.pipeline(\n","            prompt,\n","            max_length=max_length,\n","            num_return_sequences=1,\n","            pad_token_id=self.pipeline.tokenizer.eos_token_id\n","        )\n","\n","        # Extraer solo la parte nueva generada\n","        generated_text = response[0]['generated_text']\n","        new_text = generated_text[len(prompt):].strip()\n","\n","        return new_text\n"],"metadata":{"id":"5RlXYTvdheZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inicializar LLM\n","llm_manager = LLMManager()"],"metadata":{"id":"dqM2oWzvhuXK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7. Sistema RAG Básico"],"metadata":{"id":"qASFF3mWiuNj"}},{"cell_type":"code","source":["class RAGSystem:\n","    def __init__(self, vector_db: VectorDatabase, llm_manager: LLMManager):\n","        self.vector_db = vector_db\n","        self.llm_manager = llm_manager\n","        self.tortured_phrases = {\n","            \"bosom malignancy\": \"breast cancer\",\n","            \"fake patterns\": \"actual patterns\",\n","            \"artificial results\": \"research results\"\n","        }\n","\n","    def retrieve_context(self, query: str, k: int = 3) -> List[Document]:\n","        \"\"\"Recupera contexto relevante\"\"\"\n","        return self.vector_db.similarity_search(query, k=k)\n","\n","    def create_prompt(self, query: str, context_docs: List[Document]) -> str:\n","        \"\"\"Crea prompt con contexto\"\"\"\n","        context = \"\\n\\n\".join([\n","            f\"Document {i+1}: {doc.page_content}\"\n","            for i, doc in enumerate(context_docs)\n","        ])\n","\n","        prompt = f\"\"\"Based on the following scientific documents, please answer the question accurately.\n","\n","Context:\n","{context}\n","\n","Question: {query}\n","\n","Answer:\"\"\"\n","\n","        return prompt\n","\n","    def generate_rag_response(self, query: str) -> Dict[str, Any]:\n","        \"\"\"Genera respuesta completa RAG\"\"\"\n","        # 1. Recuperar contexto\n","        context_docs = self.retrieve_context(query)\n","\n","        # 2. Crear prompt\n","        prompt = self.create_prompt(query, context_docs)\n","\n","        # 3. Generar respuesta\n","        response = self.llm_manager.generate_response(prompt)\n","\n","        # 4. Preparar resultado\n","        result = {\n","            'query': query,\n","            'response': response,\n","            'context_docs': context_docs,\n","            'sources': [doc.metadata for doc in context_docs],\n","            'prompt_used': prompt\n","        }\n","\n","        return result\n","\n","# Inicializar sistema RAG\n","rag_system = RAGSystem(vector_db, llm_manager)\n"],"metadata":{"id":"Aa46jJg5isE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 4: Componente XAI (Explicabilidad)"],"metadata":{"id":"DTfHydgIi9sV"}},{"cell_type":"markdown","source":["### 8. Detector de Hallucinations y Tortured Phrases"],"metadata":{"id":"dVPCVN98jAlC"}},{"cell_type":"code","source":["class FactChecker:\n","    def __init__(self):\n","        self.tortured_phrases = {\n","            \"bosom malignancy\": {\n","                \"correct\": \"breast cancer\",\n","                \"confidence\": 0.95,\n","                \"explanation\": \"Término médico no estándar que puede indicar contenido generado automáticamente\"\n","            },\n","            \"fake patterns\": {\n","                \"correct\": \"patterns\",\n","                \"confidence\": 0.85,\n","                \"explanation\": \"Uso de 'fake' puede indicar contenido poco confiable\"\n","            }\n","        }\n","\n","        self.medical_terms_validation = {\n","            \"breast cancer\": True,\n","            \"machine learning\": True,\n","            \"bosom malignancy\": False\n","        }\n","\n","    def detect_tortured_phrases(self, text: str) -> List[Dict]:\n","        \"\"\"Detecta frases problemáticas\"\"\"\n","        detections = []\n","\n","        for phrase, info in self.tortured_phrases.items():\n","            if phrase.lower() in text.lower():\n","                detection = {\n","                    'phrase': phrase,\n","                    'position': text.lower().index(phrase.lower()),\n","                    'severity': 'high' if info['confidence'] > 0.9 else 'medium',\n","                    'suggested_correction': info['correct'],\n","                    'explanation': info['explanation'],\n","                    'confidence': info['confidence']\n","                }\n","                detections.append(detection)\n","\n","        return detections\n","\n","    def validate_against_sources(self, response: str, source_docs: List[Document]) -> Dict:\n","        \"\"\"Valida respuesta contra fuentes\"\"\"\n","        source_text = \" \".join([doc.page_content for doc in source_docs])\n","\n","        # Análisis simple de overlap\n","        response_words = set(response.lower().split())\n","        source_words = set(source_text.lower().split())\n","\n","        overlap = response_words.intersection(source_words)\n","        overlap_ratio = len(overlap) / len(response_words) if response_words else 0\n","\n","        validation = {\n","            'overlap_ratio': overlap_ratio,\n","            'supported': overlap_ratio > 0.3,  # Umbral simple\n","            'unsupported_words': response_words - source_words,\n","            'explanation': f\"La respuesta tiene {overlap_ratio:.2%} de palabras respaldadas por las fuentes\"\n","        }\n","\n","        return validation\n","\n","    def comprehensive_check(self, response: str, source_docs: List[Document]) -> Dict:\n","        \"\"\"Verificación completa\"\"\"\n","        # Detectar tortured phrases\n","        tortured_detections = self.detect_tortured_phrases(response)\n","\n","        # Validar contra fuentes\n","        source_validation = self.validate_against_sources(response, source_docs)\n","\n","        # Calcular score de confiabilidad\n","        reliability_score = 1.0\n","\n","        # Penalizar por tortured phrases\n","        for detection in tortured_detections:\n","            penalty = detection['confidence'] * 0.3\n","            reliability_score -= penalty\n","\n","        # Penalizar por bajo respaldo de fuentes\n","        if not source_validation['supported']:\n","            reliability_score -= 0.4\n","\n","        reliability_score = max(0.0, reliability_score)\n","\n","        return {\n","            'tortured_phrases': tortured_detections,\n","            'source_validation': source_validation,\n","            'reliability_score': reliability_score,\n","            'is_reliable': reliability_score > 0.6,\n","            'warnings': self._generate_warnings(tortured_detections, source_validation)\n","        }\n","\n","    def _generate_warnings(self, tortured_detections: List, source_validation: Dict) -> List[str]:\n","        \"\"\"Genera advertencias para el usuario\"\"\"\n","        warnings = []\n","\n","        if tortured_detections:\n","            warnings.append(f\"⚠️ Detectadas {len(tortured_detections)} frases problemáticas\")\n","\n","        if not source_validation['supported']:\n","            warnings.append(\"⚠️ Respuesta poco respaldada por fuentes originales\")\n","\n","        return warnings\n","\n","# Inicializar fact checker\n","fact_checker = FactChecker()"],"metadata":{"id":"AzU6dF_ajDiR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9. Sistema de Explicabilidad con SHAP/LIME\n"],"metadata":{"id":"ylfLAPtsjRlp"}},{"cell_type":"code","source":["class ExplainabilityEngine:\n","    def __init__(self, rag_system: RAGSystem):\n","        self.rag_system = rag_system\n","        self.lime_explainer = LimeTextExplainer(class_names=['reliable', 'unreliable'])\n","\n","    def explain_response_sources(self, query: str, response: str, source_docs: List[Document]) -> Dict:\n","        \"\"\"Explica qué fuentes contribuyeron a la respuesta\"\"\"\n","        explanations = []\n","\n","        for i, doc in enumerate(source_docs):\n","            # Calcular similitud simple entre respuesta y documento\n","            doc_words = set(doc.page_content.lower().split())\n","            response_words = set(response.lower().split())\n","\n","            common_words = doc_words.intersection(response_words)\n","            influence_score = len(common_words) / len(response_words) if response_words else 0\n","\n","            explanation = {\n","                'source_id': i,\n","                'title': doc.metadata.get('title', f'Document {i+1}'),\n","                'influence_score': influence_score,\n","                'common_concepts': list(common_words)[:10],  # Top 10\n","                'metadata': doc.metadata\n","            }\n","            explanations.append(explanation)\n","\n","        # Ordenar por influencia\n","        explanations.sort(key=lambda x: x['influence_score'], reverse=True)\n","\n","        return {\n","            'source_influences': explanations,\n","            'most_influential': explanations[0] if explanations else None,\n","            'explanation_summary': self._create_influence_summary(explanations)\n","        }\n","\n","    def explain_reliability_factors(self, fact_check_result: Dict) -> Dict:\n","        \"\"\"Explica factores que afectan la confiabilidad\"\"\"\n","        factors = []\n","\n","        # Factor: Tortured phrases\n","        if fact_check_result['tortured_phrases']:\n","            factor = {\n","                'type': 'tortured_phrases',\n","                'impact': 'negative',\n","                'weight': 0.3,\n","                'description': f\"Detectadas {len(fact_check_result['tortured_phrases'])} frases problemáticas\",\n","                'details': fact_check_result['tortured_phrases']\n","            }\n","            factors.append(factor)\n","\n","        # Factor: Source support\n","        source_support = fact_check_result['source_validation']['supported']\n","        factor = {\n","            'type': 'source_support',\n","            'impact': 'positive' if source_support else 'negative',\n","            'weight': 0.4,\n","            'description': f\"Respaldo de fuentes: {'Alto' if source_support else 'Bajo'}\",\n","            'details': fact_check_result['source_validation']\n","        }\n","        factors.append(factor)\n","\n","        return {\n","            'reliability_factors': factors,\n","            'overall_score': fact_check_result['reliability_score'],\n","            'recommendation': self._get_reliability_recommendation(fact_check_result['reliability_score'])\n","        }\n","\n","    def _create_influence_summary(self, explanations: List[Dict]) -> str:\n","        \"\"\"Crea resumen de influencias\"\"\"\n","        if not explanations:\n","            return \"No se encontraron fuentes influyentes\"\n","\n","        top_source = explanations[0]\n","        return f\"La fuente más influyente es '{top_source['title']}' con {top_source['influence_score']:.2%} de influencia\"\n","\n","    def _get_reliability_recommendation(self, score: float) -> str:\n","        \"\"\"Obtiene recomendación basada en score\"\"\"\n","        if score > 0.8:\n","            return \"✅ Alta confiabilidad - Información respaldada por fuentes\"\n","        elif score > 0.6:\n","            return \"⚠️ Confiabilidad moderada - Verificar información importante\"\n","        else:\n","            return \"❌ Baja confiabilidad - Consultar fuentes adicionales\"\n","\n","# Inicializar motor de explicabilidad\n","explainability_engine = ExplainabilityEngine(rag_system)"],"metadata":{"id":"MrByOmyejSgB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 5: Sistema Integrado XAI-RAG"],"metadata":{"id":"uOw2g2pmjd8C"}},{"cell_type":"markdown","source":["### 10. Sistema Principal"],"metadata":{"id":"q-kTgMvbjggC"}},{"cell_type":"code","source":["class FactCheckXAIRAG:\n","    def __init__(self, rag_system: RAGSystem, fact_checker: FactChecker,\n","                 explainability_engine: ExplainabilityEngine):\n","        self.rag_system = rag_system\n","        self.fact_checker = fact_checker\n","        self.explainability_engine = explainability_engine\n","\n","    def process_query(self, query: str) -> Dict[str, Any]:\n","        \"\"\"Procesa query completo con XAI\"\"\"\n","        print(f\"🔄 Procesando: {query}\")\n","\n","        # 1. Generar respuesta RAG\n","        rag_result = self.rag_system.generate_rag_response(query)\n","\n","        # 2. Verificar hechos\n","        fact_check_result = self.fact_checker.comprehensive_check(\n","            rag_result['response'],\n","            rag_result['context_docs']\n","        )\n","\n","        # 3. Generar explicaciones\n","        source_explanation = self.explainability_engine.explain_response_sources(\n","            query,\n","            rag_result['response'],\n","            rag_result['context_docs']\n","        )\n","\n","        reliability_explanation = self.explainability_engine.explain_reliability_factors(\n","            fact_check_result\n","        )\n","\n","        # 4. Compilar resultado completo\n","        complete_result = {\n","            'query': query,\n","            'response': rag_result['response'],\n","            'sources': rag_result['sources'],\n","            'fact_check': fact_check_result,\n","            'explanations': {\n","                'source_influence': source_explanation,\n","                'reliability_factors': reliability_explanation\n","            },\n","            'metadata': {\n","                'timestamp': datetime.now().isoformat(),\n","                'model_used': self.rag_system.llm_manager.model_name,\n","                'total_sources': len(rag_result['context_docs'])\n","            }\n","        }\n","\n","        return complete_result\n","\n","    def format_response_for_display(self, result: Dict) -> str:\n","        \"\"\"Formatea respuesta para mostrar al usuario\"\"\"\n","        output = []\n","\n","        # Respuesta principal\n","        output.append(f\"**Respuesta:**\\n{result['response']}\\n\")\n","\n","        # Advertencias si las hay\n","        warnings = result['fact_check']['warnings']\n","        if warnings:\n","            output.append(\"**⚠️ Advertencias:**\")\n","            for warning in warnings:\n","                output.append(f\"- {warning}\")\n","            output.append(\"\")\n","\n","        # Score de confiabilidad\n","        reliability = result['fact_check']['reliability_score']\n","        recommendation = result['explanations']['reliability_factors']['recommendation']\n","        output.append(f\"**Confiabilidad:** {reliability:.2%}\")\n","        output.append(f\"{recommendation}\\n\")\n","\n","        # Fuentes más influyentes\n","        most_influential = result['explanations']['source_influence']['most_influential']\n","        if most_influential:\n","            output.append(f\"**Fuente principal:** {most_influential['title']}\")\n","            output.append(f\"**Influencia:** {most_influential['influence_score']:.2%}\\n\")\n","\n","        # Tortured phrases detectadas\n","        tortured = result['fact_check']['tortured_phrases']\n","        if tortured:\n","            output.append(\"**🔍 Frases problemáticas detectadas:**\")\n","            for detection in tortured:\n","                output.append(f\"- '{detection['phrase']}' → Sugerencia: '{detection['suggested_correction']}'\")\n","                output.append(f\"  {detection['explanation']}\")\n","            output.append(\"\")\n","\n","        return \"\\n\".join(output)\n","\n","# Inicializar sistema completo\n","xai_rag_system = FactCheckXAIRAG(rag_system, fact_checker, explainability_engine)"],"metadata":{"id":"4GCa-gmjjiiL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 6: Interfaz de Usuario con Gradio"],"metadata":{"id":"jJOmHccMmB_t"}},{"cell_type":"markdown","source":["### 11. Interfaz Web Interactiva"],"metadata":{"id":"P6KX-KilmD-b"}},{"cell_type":"code","source":["def create_gradio_interface():\n","    \"\"\"Crea interfaz Gradio para el sistema\"\"\"\n","\n","    def process_user_query(query, show_detailed_analysis):\n","        \"\"\"Procesa query del usuario\"\"\"\n","        if not query.strip():\n","            return \"Por favor, ingresa una pregunta.\", \"\"\n","\n","        try:\n","            # Procesar con el sistema XAI-RAG\n","            result = xai_rag_system.process_query(query)\n","\n","            # Respuesta formateada para mostrar\n","            formatted_response = xai_rag_system.format_response_for_display(result)\n","\n","            # Análisis detallado si se solicita\n","            detailed_analysis = \"\"\n","            if show_detailed_analysis:\n","                detailed_analysis = format_detailed_analysis(result)\n","\n","            return formatted_response, detailed_analysis\n","\n","        except Exception as e:\n","            return f\"❌ Error procesando la consulta: {str(e)}\", \"\"\n","\n","    def format_detailed_analysis(result):\n","        \"\"\"Formatea análisis detallado\"\"\"\n","        analysis = []\n","\n","        analysis.append(\"## 📊 Análisis Detallado\\n\")\n","\n","        # Información de fuentes\n","        analysis.append(\"### 📚 Fuentes Utilizadas:\")\n","        for i, source in enumerate(result['sources']):\n","            analysis.append(f\"{i+1}. **{source.get('title', 'Sin título')}**\")\n","            if 'authors' in source:\n","                analysis.append(f\"   - Autores: {source['authors']}\")\n","            if 'published' in source:\n","                analysis.append(f\"   - Publicado: {source['published']}\")\n","            analysis.append(\"\")\n","\n","        # Influencia de fuentes\n","        source_influences = result['explanations']['source_influence']['source_influences']\n","        analysis.append(\"### 🎯 Influencia de Fuentes:\")\n","        for influence in source_influences[:3]:  # Top 3\n","            analysis.append(f\"- **{influence['title']}**: {influence['influence_score']:.2%} influencia\")\n","\n","        # Factores de confiabilidad\n","        analysis.append(\"\\n### 🔍 Factores de Confiabilidad:\")\n","        reliability_factors = result['explanations']['reliability_factors']['reliability_factors']\n","        for factor in reliability_factors:\n","            icon = \"✅\" if factor['impact'] == 'positive' else \"❌\"\n","            analysis.append(f\"{icon} **{factor['type']}**: {factor['description']}\")\n","\n","        return \"\\n\".join(analysis)\n","\n","    # Crear interfaz\n","    with gr.Blocks(title=\"FactCheck XAI-RAG\", theme=gr.themes.Soft()) as interface:\n","        gr.Markdown(\"\"\"\n","        # 🔬 FactCheck XAI-RAG\n","        ### Sistema de Verificación de Hechos Explicable para Preguntas Científicas\n","\n","        Este sistema utiliza RAG (Retrieval-Augmented Generation) con explicabilidad (XAI) para:\n","        - ✅ Responder preguntas científicas con fuentes\n","        - 🔍 Detectar información potencialmente incorrecta\n","        - 📊 Explicar el proceso de generación de respuestas\n","        - ⚠️ Alertar sobre contenido poco confiable\n","        \"\"\")\n","\n","        with gr.Row():\n","            with gr.Column(scale=2):\n","                query_input = gr.Textbox(\n","                    label=\"💬 Tu pregunta científica\",\n","                    placeholder=\"Ej: ¿Qué avances recientes hay en machine learning para detección de cáncer?\",\n","                    lines=3\n","                )\n","\n","                show_detailed = gr.Checkbox(\n","                    label=\"📊 Mostrar análisis detallado\",\n","                    value=False\n","                )\n","\n","                submit_btn = gr.Button(\"🔍 Analizar\", variant=\"primary\")\n","\n","            with gr.Column(scale=1):\n","                gr.Markdown(\"\"\"\n","                **💡 Ejemplos de preguntas:**\n","                - \"¿Qué es la bosom malignancy?\"\n","                - \"¿Cómo funciona machine learning en medicina?\"\n","                - \"¿Cuáles son los tratamientos para breast cancer?\"\n","                \"\"\")\n","\n","        with gr.Row():\n","            with gr.Column():\n","                response_output = gr.Markdown(label=\"📋 Respuesta y Análisis\")\n","\n","            with gr.Column():\n","                detailed_output = gr.Markdown(label=\"📊 Análisis Detallado\", visible=False)\n","\n","        # Mostrar/ocultar análisis detallado\n","        def toggle_detailed(show_detailed):\n","            return gr.update(visible=show_detailed)\n","\n","        show_detailed.change(\n","            fn=toggle_detailed,\n","            inputs=[show_detailed],\n","            outputs=[detailed_output]\n","        )\n","\n","        # Procesar consulta\n","        submit_btn.click(\n","            fn=process_user_query,\n","            inputs=[query_input, show_detailed],\n","            outputs=[response_output, detailed_output]\n","        )\n","\n","        # Ejemplos interactivos\n","        gr.Examples(\n","            examples=[\n","                [\"¿Qué avances hay en machine learning para detección de bosom malignancy?\", True],\n","                [\"¿Cómo funciona la inteligencia artificial en medicina?\", False],\n","                [\"¿Cuáles son los síntomas del breast cancer?\", True]\n","            ],\n","            inputs=[query_input, show_detailed]\n","        )\n","\n","    return interface\n"],"metadata":{"id":"knW8zdb7mAC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear y lanzar interfaz\n","print(\"🚀 Creando interfaz web...\")\n","interface = create_gradio_interface()"],"metadata":{"id":"pVclctNYmYZE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fase 7: Lanzamiento y Pruebas"],"metadata":{"id":"BH2dzTp3mBE8"}},{"cell_type":"markdown","source":["### 12. Configuración Final y Lanzamiento"],"metadata":{"id":"zdFf4w_RnRq3"}},{"cell_type":"code","source":["# Configurar ngrok para acceso público (opcional)\n","def setup_public_access():\n","    \"\"\"Configura acceso público con ngrok\"\"\"\n","    try:\n","        # Configurar token de ngrok si tienes uno\n","        # ngrok.set_auth_token(\"tu_token_aqui\")\n","\n","        # Lanzar túnel\n","        public_url = ngrok.connect(7860)\n","        print(f\"🌐 URL pública: {public_url}\")\n","        return public_url\n","    except Exception as e:\n","        print(f\"⚠️ No se pudo configurar acceso público: {e}\")\n","        return None\n"],"metadata":{"id":"pFZXVYqgnT99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Función principal de lanzamiento\n","def launch_system():\n","    \"\"\"Lanza el sistema completo\"\"\"\n","    print(\"🚀 Iniciando FactCheck XAI-RAG...\")\n","\n","    # Verificar que todo esté inicializado\n","    components_status = {\n","        'Vector Database': vectorstore is not None,\n","        'LLM Manager': llm_manager.pipeline is not None,\n","        'RAG System': rag_system is not None,\n","        'Fact Checker': fact_checker is not None,\n","        'XAI Engine': explainability_engine is not None,\n","        'Main System': xai_rag_system is not None\n","    }\n","\n","    print(\"\\n📋 Estado de componentes:\")\n","    for component, status in components_status.items():\n","        status_icon = \"✅\" if status else \"❌\"\n","        print(f\"{status_icon} {component}\")\n","\n","    if not all(components_status.values()):\n","        print(\"❌ Error: Algunos componentes no están inicializados\")\n","        return False\n","\n","    # Configurar acceso público (opcional)\n","    # public_url = setup_public_access()\n","\n","    # Lanzar interfaz\n","    print(\"\\n🌐 Lanzando interfaz web...\")\n","    interface.launch(\n","        server_name=\"0.0.0.0\",\n","        server_port=7860,\n","        share=True,  # Esto crea un enlace público temporal\n","        debug=True\n","    )\n","\n","    return True\n"],"metadata":{"id":"XGdCzGH-ns0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Función de pruebas rápidas\n","def run_quick_tests():\n","    \"\"\"Ejecuta pruebas rápidas del sistema\"\"\"\n","    print(\"🧪 Ejecutando pruebas rápidas...\\n\")\n","\n","    test_queries = [\n","        \"¿Qué es la bosom malignancy?\",  # Debería detectar tortured phrase\n","        \"¿Cómo funciona machine learning en medicina?\",  # Pregunta normal\n","        \"¿Cuáles son los tratamientos para breast cancer?\"  # Pregunta médica válida\n","    ]\n","\n","    for i, query in enumerate(test_queries, 1):\n","        print(f\"🔬 Prueba {i}: {query}\")\n","        try:\n","            result = xai_rag_system.process_query(query)\n","\n","            # Mostrar resultados clave\n","            print(f\"   📝 Respuesta: {result['response'][:100]}...\")\n","            print(f\"   🎯 Confiabilidad: {result['fact_check']['reliability_score']:.2%}\")\n","            print(f\"   ⚠️ Advertencias: {len(result['fact_check']['warnings'])}\")\n","            print(f\"   📚 Fuentes: {len(result['sources'])}\")\n","\n","            if result['fact_check']['tortured_phrases']:\n","                print(f\"   🔍 Tortured phrases: {len(result['fact_check']['tortured_phrases'])}\")\n","\n","            print()\n","\n","        except Exception as e:\n","            print(f\"   ❌ Error: {e}\\n\")\n","\n","    print(\"✅ Pruebas completadas\")\n"],"metadata":{"id":"QlzmWCXtn_1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ejecutar pruebas primero\n","run_quick_tests()"],"metadata":{"id":"8nrFH_tQoIhF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lanzar sistema\n","if launch_system():\n","    print(\"\"\"\n","    🎉 ¡Sistema FactCheck XAI-RAG lanzado exitosamente!\n","\n","    📱 Usa la interfaz web para:\n","    - Hacer preguntas científicas\n","    - Ver explicaciones detalladas\n","    - Identificar información poco confiable\n","\n","    🔬 Características principales activas:\n","    ✅ Detección de tortured phrases\n","    ✅ Verificación contra fuentes\n","    ✅ Explicabilidad de respuestas\n","    ✅ Scoring de confiabilidad\n","    ✅ Interfaz intuitiva\n","    \"\"\")\n","else:\n","    print(\"❌ Error al lanzar el sistema\")\n"],"metadata":{"id":"hMyGyM0KoSLn"},"execution_count":null,"outputs":[]}]}